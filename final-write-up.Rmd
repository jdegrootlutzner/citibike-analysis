---
title: "Citi Bikes Analysis and Bike Demand Prediction"
author: "Julian DeGroot-Lutzner & Adi Salwan"
date: "12/14/2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo= TRUE, message = FALSE, warning = FALSE,
                      fig.width = 7, fig.height = 4,
                      fig.align = "center")
options(digits=4)

require(dplyr)
require(ggplot2)
require(RMySQL)
require(lubridate)
require(ggmap)
require(readr)
require(chron)
require(caret)
```
```{r, include=FALSE, cache = TRUE}
randomsample <- read_csv("data/randomsample.csv")

```
## R Markdown
 <http://rmarkdown.rstudio.com>. **Knit** 
 
 
 
## Background and Motivation

Our project is inspired by the [Kaggle bike sharing demand competition](https://www.kaggle.com/c/bike-sharing-demand#description). Bike sharing is a service that has become popular within the last few years in many cities across the United States. Bikes are checked in and out of a network of stations that keep track of each ride, its start and end time, its start/end location, as well as information about subscribers such as gender and age. The Kaggle competition challenged users to predict the hourly bike usage of a test dataset by building a model on a training set combing hourly weather data with hourly bike share usage in Washington, DC.

We planned to first visualize and understand the data and then try to find if there are any inconsistencies in bike stations traffic. Is there a bike station that people take bikes from but do not take bikes back to? If there was an bike station that is a net exporter - meaning it has an excess in demand of bikes - we wanted to make recomendations on how often CitiBikes should manually transport bikes back to the station.

Instead of using the Kaggle data, we wanted to wrangle bike sharing data from a different city's bike share dataset so that we could get familiar with the process of preparing a dataset for analysis. We decided on the [Citibikes bike sharing dataset](https://www.citibikenyc.com/system-data) from New York City because there is data available on every ride from January 1st, 2013 to September 31, 2017. However, the dataset is very large, over 40 million observations, and the complete set cannot be ran in R. Our new goal became learning how to work with Big Data in R. 


## Working with Big Data in R

Big Data in R is when the data cannot fit in to memory. Instead, we stored the data in a SQL database and received the data using 'RMySQL' and 'dplyr.' We based our work process off of the [Working with Big Data in R webinar.](https://www.rstudio.com/resources/webinars/working-with-big-data-in-r/). The life cycle of a big data analysis project usually involves five parts. Subset (extract data to explore and work with), clarify (become familiar with the data and template a solution), develop (create a working model), productize (automate and integrate), and publish. In other words, first work with a smaller sample, then scale up the work to a larger dataset.

We worked on Jo Hardin's sever at the Pomona math department using MySQL. Since the server is within the Pomona network we needed to navigate both the Pomona ITS and Department firewall. Additonally, our limited permissions on our personal accounts made it difficult to update needed packages within R. These limitations forced us to learn, adapt, and change our project as needed.

The first difficulty in the project was uploading the Citibikes data to a SQL database. Citibikes publishes a zip file for each month of data so there were over 50 seperate files to download. We wrote scripts to download, alter, and write csv files from the the Citibikes server, to our server, and then onto the MySQL database. The difficulties served as a learning experience as we developed better skills in the command line as well as in writing Shell, Python, and SQL code. You can see some of this code in the setup-code folder. Unfortunately, as of now our complete process is not reproducible by a single code but we can come back and add more explanation on how to use this code later.

Dplyr allows integration with many different databases. We originally tried running code on the complete >40 million dataset. However, the SQL queries took too long for us to create a reasonable project within our relatively short time frame. For example, some queries would take more than a day. In the future we could use other big data techniques like MapReduce to work more quickly. Instead we used the aforementioned big data work schedule of subsetting and scaling. First we took a random sample of 500,000 observations - approxiametly 1 % - from the original dataset.

## Understanding the Random Sample

### Exploratory Graphs on Ride Use

We wrote a lot of code, but we decided to use as little code as possible in our write up so that it is easier to read. If you're interested in learning how to make cool graphs like these please download our Rmd file and take a look for yourself. 

Here are some graphs that should help you get an understanding of CitiBike usage. After the graphs is a write up about some of the takeaways we found.

```{r, cache =TRUE, echo=FALSE}
# Parsing all the start times into one format
mdy <- mdy_hms(randomsample$starttime) 
ymd <- ymd_hms(randomsample$starttime) 
f1 <- mdy_hm(randomsample$starttime)
mdy[is.na(mdy)] <- ymd[is.na(mdy)] # some dates are ambiguous, here we give 
randomsample$starttime <- mdy
randomsample$starttime[is.na(randomsample$starttime)] <- f1[is.na(randomsample$starttime)]

# Parsing all the end times into one format 
mdy <- mdy_hms(randomsample$stoptime) 
ymd <- ymd_hms(randomsample$stoptime) 
f1 <- mdy_hm(randomsample$stoptime)
mdy[is.na(mdy)] <- ymd[is.na(mdy)] 
randomsample$stoptime <- mdy
randomsample$stoptime[is.na(randomsample$stoptime)] <- f1[is.na(randomsample$stoptime)]

# Flooring the Hour
randomsample$starttime <- floor_date(randomsample$starttime,unit = "hour")
# Trying to Find Day of the Week
randomsample <- randomsample %>%
  mutate(WeekDay=wday(as.Date(starttime), label=TRUE)) %>%
  mutate(WorkingDay=ifelse((WeekDay=="Mon"|WeekDay=="Tues"|WeekDay=="Wed"|WeekDay=="Thurs"|WeekDay=="Fri"),"Yes","No")) 

# Trying to find the holidays - Not quite working. Need to do this manually!
randomsample <- randomsample %>%
  mutate(holiday=is.weekend(starttime))

# Joining the weather data and the random sample
combined <- randomsample

# Number of Stations Every Year
rs <- randomsample %>%
  mutate(year=year(starttime),monyear=substr(starttime,1,7)) %>%
  group_by(year,usertype,monyear) %>%
  summarise(stations=n_distinct(start.station.id),rides=n())
combined$WorkingDay[combined$WorkingDay=="Yes"]<- "Weekday"
combined$WorkingDay[combined$WorkingDay=="No"]<- "Weekend"

# Exploratory Analysis -
combined1 <- combined%>% 
  mutate(year=year(starttime),
         month=month(starttime),
         monyear=substr(starttime,1,7)) %>%
  group_by(monyear,year)%>%
  summarise(usage=n())

combined1$year <- as.factor(combined1$year)

# Graph to See the Monthly Usage
ggplot(combined1,aes(x=monyear,y=usage,fill=year))+
  geom_bar(stat="identity")+ 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+
  ggtitle("Bike Usage in Different Months (July 2013 to September 2017)")+
  ylab("Number of Bike Rides")+
  xlab("Month and Year")

# Wrangling the data
combined2 <-  combined%>% 
  mutate(year=year(starttime),
         week=week(starttime),
         monyear=substr(starttime,1,7)) %>%
  group_by(week,year)%>%
  summarise(usage=n(),MedianDuration=median(tripduration)) %>%
  mutate(wy=paste(week,"-",year)) %>%
  filter(year==2016) %>%
  arrange(year)

# Graph to See the Usage in 2016
ggplot(combined2,
       aes(x=week,
           y=usage,
           fill=MedianDuration))+
  geom_bar(stat="identity")+
  theme(axis.text.x = 
          element_text(angle = 90, hjust = 1))+
  ggtitle("Weekly Bike Rides in 2016")+ylab("Usage")+xlab("Week")

combined4 <- combined%>% 
  mutate(year=year(starttime),
         month=month(starttime),
         hour=hour(starttime)) %>%
  filter(!is.na(usertype))%>%
  group_by(hour,gender,usertype,WeekDay)%>%
  summarise(usage=n(),MedianDuration=median(tripduration))
# The Number of Rides for every hour 

#Graph for Bike Usage on Different Day
ggplot(combined4,aes(x=hour,y=usage,fill=WeekDay))+
  geom_bar(stat="identity")+
  facet_grid(~WeekDay)+
  xlab("Hour of the Day")+
  ylab("Number of Bike Rides")+
  ggtitle("Bike Usage on Different Days")

combined7 <- combined%>% 
  mutate(year=year(starttime),month=month(starttime),hour=hour(starttime)) %>%
  filter(!is.na(usertype))%>%
  group_by(hour,gender,usertype,WorkingDay)%>%
  summarise(usage=n(),MedianDuration=median(tripduration))
# The Number of Rides for every hour - 

ggplot(combined7,aes(x=hour,y=usage,fill=WorkingDay))+
  geom_bar(stat="identity")+
  facet_grid(~WorkingDay)+
  scale_fill_manual(values=c("orange", "lightgreen"))+
  xlab("Hour of the Day")+
  ylab("Number of Bike Rides")+
  ggtitle("Bike Usage on Weekdays versus Weekends")

# Making Different Levels for the Time of the Day
combined4 <- combined4 %>%
  mutate(EarlyMorning=ifelse(hour=="3"|hour=="4"|hour=="5"|hour=="6",1,0)) %>%
  mutate(Commuting=ifelse((hour=="7"|hour=="8"|hour=="9"),1,0)) %>%
  mutate(DayTime=ifelse((hour=="10"|hour=="11"|hour=="12"|hour=="13"|hour=="14"|hour=="15"),1,0)) %>%
  mutate(Evening=ifelse((hour=="16"|hour=="17"|hour=="18"|hour=="19"|hour=="20"),1,0)) %>%
  mutate(Night=ifelse((hour=="21"|hour=="22"|hour=="23"|hour=="0"|hour=="1"|hour=="2"),1,0))

combined5 <- combined%>% 
    filter(!is.na(birth.year)) %>%
  mutate(year=year(starttime),
         month=month(starttime),
         monyear=substr(starttime,1,7),
         age = 2017-birth.year) %>%
  group_by(monyear,year,age,usertype,gender)%>%
  summarise(usage=n())%>%
  filter(age<100,gender!=0)

combined5$gender <- as.factor(combined5$gender)

ggplot(combined5,aes(x=age,y=usage,fill=gender))+
  geom_bar(stat="identity")+facet_grid(~year)+
  ggtitle("Yearly Bike Usage by Age and Gender")+
  ylab("Number of Bike Rides")+
  xlab("Age")+
  scale_fill_manual(values=c("skyblue", "lightgreen"))+
  geom_vline(xintercept = 75,alpha=0.5)+
  geom_vline(xintercept = 25,alpha=0.5)

ggplot(combined5,aes(x=year,y=usage,fill=gender))+
  geom_bar(stat="identity")+
  theme(axis.text.x = 
          element_text(angle = 90, hjust = 1))+
  facet_grid(~gender)+
  ggtitle("Yearly Bike Usage by Gender")+
  ylab("Number of Bike Rides")+
  xlab("Year")+
  scale_fill_manual(values=c("skyblue", "lightpink"))
```

Based on a preliminary exploratory analysis the following insights were found which could be useful for Citi Bikes: 

1) It was found that bike usage for both females and males has increased each year since the initiation of this bike sharing project in May 2013 and since the data became available in July 2013. After the missing observations for gender were removed, it was found that bikes are used predominantly by males. We believe this trend is unexpected and needs to be corrected. We posit that female prefer walking, which is why the bike usage for females is not very high as compared to bike usage for males. The 2017 data is only available till September, which is why when compared to 2016 it shows a decline.

2) It was found that bike usage has increased the most for the 25-50 years age group. The bike usage for the age group of 50-75 has increased significantly too, while the usage for the age group less than 25 increased too. The bike usage for the age group of 75 years or older has not increased much. Based on this information, we believe that the target market for Citi Bikes is 25 to 50 year olds. 

3) In the year 2016 bike usage peaked from week 21 to week 44. This corresponds to the dates from May 16 to October 31. This is because bike usage is generally higher during the summer and fall months. It was also found that the median bike ride is longer during these months as the weather does not act as impediment for bikers. The highest median bike ride was in week 36 (12 minutes and 9 seconds), while the lowest was in week 7 (8 minutes and 8 seconds). Hence weather does play a part in biker's decision whether to bike or not. However, it remains to be seen which of the six weather predictors play the most important role in a user's decision to bike or not. 

4) Based on the first graph, it can be seen that the overall popularity of this bike sharing system has surged significantly each year. It has increased each year since its inception. The number of bike rides continue to grow, but the number of bike rides in the last 4 months of the years are higher in 2013 than in 2014, albeit not by a big amount. This may have been because of the weather conditions. The average temperature in September 2013 was 67.2 degree Farenheit, 2 degrees higher than the average temperature in September 2014, which could have translated in the marginally lower number of bike rides in in September 2014 than in September 2013. Similarly, the average temperature in December 2013 was 2 degrees lower than the average temperature in December 2014. We hypothesize that weather conditions could be the reason for this slight decrease in bike rentals. However, it is still not quite clear about why the number of bike rides rented in October and November 2013 were marginally lower than in October and November 2014. 

5) The bike usage is significantly higher during the weekdays than during weekends. This could be because the bikes are predominantly used by commuters commuting to work on weekdays. This drop is the highest during the commuting hours from 7am to 9am and from 4pm to 7pm, which makes us believe that commuters account for a majority of the bike rides in New York City. Based on this information, Citi Bikes could shift its major focus to commuters instead of any other group. This could be undertaking or promoting campaigns to bike to work, which would increase their annual memberships. 

It is important to note that the exploratory analysis is based on a sample of 500,000 observations, which is 1% of the total population. This sample is representative of the population because of its large size. 

## Station Analysis

After getting a better sense of the data, we will return to one of the orignal questions we had. Are there any bike stations that have inconsistencies between bike supply and bike demand? This part of the exploratory analysis moves to the stations. 

First lets see where the stations are located in New York City.

```{r,cache=TRUE, echo=FALSE}
# wrangling data for graphs
start_sums <- randomsample %>%
  group_by(start.station.id) %>%
  summarize(start.station.longitude = mean(start.station.longitude),
            start.station.latitude = mean(start.station.latitude),
            median.time.out = median(tripduration, na.rm= TRUE),
            start.count = n()) %>% 
  ungroup()

# the map used for plots
center.citibikes <- c(
  lon = mean(randomsample$start.station.longitude),
  lat = mean(randomsample$start.station.latitude))
mymap <- get_map(location = center.citibikes,
                 maptype = "roadmap",
                 zoom = 12)
# heatmap
 ggmap(mymap, extent = "panel", maprange=FALSE) +
  geom_density2d(data = start_sums, 
                           aes(x = start.station.longitude,
                               y = start.station.latitude)) +
  stat_density2d(data = start_sums, 
                           aes(x = start.station.longitude,
                               y = start.station.latitude, fill = ..level.., alpha = ..level..),
                 size = 0.01, bins = 16, geom = 'polygon') +
  scale_fill_gradient(low = "green", high = "red") +
  scale_alpha(range = c(0.00, 0.25), guide = FALSE) +
  theme(legend.position = "none", axis.title = element_blank(), text = element_text(size = 12)) +
  ggtitle("Heatmap of Location of Bike Stations")
```

As you can see most bike stations are centered in lower Manhatten. Now lets get a different but related view of Citibikes as shown by the number of rides per station.
```{r, cache=TRUE, echo=FALSE}
ggmap(mymap) +  geom_point(data = start_sums, 
                           aes(x = start.station.longitude,
                               y = start.station.latitude,
                               fill = "red", alpha = start.count),
                           size = 1, shape = 21) +
  guides(fill=FALSE, alpha=FALSE, size=FALSE) +
  ggtitle("Number of Rides per Station", subtitle = "Darker Red = More Rides")
```

As we can see most bike rides happen in downtown Manhatten. The rides really slim out over the east and west side of Central park. We can see a slight increase of red in Brooklyn on the opposite side of the Brooklyn Bridge. We must keep in mind that we did not account for the number of years that the station was around. Now let's take a look at the median trip duration.
```{r, cache=TRUE, echo=FALSE}
ggmap(mymap) +  geom_point(data = start_sums, 
                           aes(x = start.station.longitude,
                               y = start.station.latitude,
                               fill = "red", 
                               alpha = median.time.out),
                           size = 1, shape = 21) +
  guides(fill=FALSE, alpha=FALSE, size=FALSE) + 
  ggtitle("Median Trip Duration to Station",
          subtitle = "Darker Red = Longer Trip")
```

The one red dot could be do to an outlier and is worth investigating more but we chose to go in a different direction.

At this point we moved in to our investigation of if a station was a net importer or exporter. To figure this out we calculated the sums of the number of trips in and out of a station for each station. We then found the difference between the start and the finish or the net change. The difference (start - finish) is positive if more bikes start at a station than end there, and is negative if more bikes end at a station than start there. **I confused myself and probably should have rethought about the change as in and out and then I would have gotten finish - start or in - out. Anyways, please deal with this confusion**

We normalized the difference by dividing it by the total station trips and we filtered the stations to only look at the top ten percent of the most popular stations (the stations with the most bike rides). Looking back at our metric, we could have adjusted for how long the bike station has been around and looked at how the usage has changed overttime. For example, did extreme popularity of one station spark a new station near it that helped provide more supply of bikes? That's a question we should look at in the future.

```{r,cache= TRUE, echo=FALSE}
end_sums <- randomsample %>%
  group_by(end.station.id) %>%
  summarize(end.station.longitude = mean(end.station.longitude),
            end.station.latitude = mean(end.station.latitude),
            median.time.in = median(tripduration, na.rm = TRUE),
            end.count = n()) %>% 
  ungroup()

colnames(start_sums)[1]<- c("id")
colnames(end_sums)[1] <- c("id")
joined_data<-inner_join(start_sums,end_sums,by="id")
joined_data <- joined_data %>%
mutate(difference = start.count - end.count,
       station.latitude = 
         (start.station.latitude+ end.station.latitude)/2,
       station.longitude = 
         (start.station.longitude + end.station.longitude)/2,
       total.ride.count = start.count + end.count) %>%
  mutate(normalized.difference =
           difference/(start.count+end.count),
         positive.difference = difference>0) %>%
  select(-start.station.latitude,
         -start.station.longitude,
         -end.station.longitude,
         -end.station.latitude) 
```

Here is a list of the top exporters - more bikes start from these stations than end at these stations. They need more bikes. 

```{r, cache = TRUE, echo=FALSE}
top_ten_perc <- joined_data$total.ride.count %>% 
  quantile(0.90)

biggest_differences <- joined_data %>% 
  filter(total.ride.count >= top_ten_perc) %>%
  arrange(desc(normalized.difference)) %>%
head(10)

biggest_differences %>% 
    select(id, start.count, end.count, 
           difference, normalized.difference)

```

Here is a list of the top importer - more bikes **end** at these stations than start in these stations. They have excess bikes.

```{r, cache=TRUE, echo=FALSE}
smallest_differences <- joined_data %>% 
  filter(total.ride.count >= top_ten_perc) %>%
  arrange(normalized.difference) %>%
head(10)
smallest_differences %>%
  select(id, start.count, end.count, 
         difference, normalized.difference)
```

Tables of data are no fun! Let's see the data visualized.

```{r, cache=TRUE, echo = FALSE}
ggmap(mymap) +
  guides(fill=FALSE, alpha=FALSE, size=FALSE) +
  geom_point(data=biggest_differences,
             aes(x = station.longitude,
                 y = station.latitude,
                 fill = "blue",
                 alpha = 1.0),
             size = 3, shape = 21) +
    geom_point(data=smallest_differences,
             aes(x = station.longitude,
                 y = station.latitude,
                 fill = "red",
                 alpha = 1.0),
             size = 3, shape = 21)
```

As you can see most of the most popular importers are in the lower side of Manhatten. Most of the popular exporters are closer to time square. Although the analysis is not perfect. We decided to move on and make a predictive models on the top two stations. We chose our importer to be station 432, which is in Alphabet City (at E 7 St & Avenue A near Washington Square Park), and our exporter to be station 521, which is in Midtown between the Empire State Building and Lincoln Tunnel (at 8 Ave & W 31 St N). Shown below:

```{r, cache = TRUE, echo=FALSE}
# Narrow down the two highest normalized difference stations
head(joined_data)
stations_of_interest <- joined_data %>% 
  filter(id == 432 | id == 521)

# Plot to show where they are
ggmap(mymap) +  
  geom_point(data=stations_of_interest,
             aes(x = station.longitude,
                 y = station.latitude,
                 fill = positive.difference,
                 alpha = 1.0),
             size = 4, shape = 21) +
  guides(fill=FALSE, alpha=FALSE, size=FALSE)  +
  ggtitle("Stations of Interest",
          subtitle = "Station 432 in Red, Station 521 in Blue")
```

Let's move on to our model.

## Building a Model
Rationale for Choosing the weather data variables:

The variables relating to the weather data chosen are air temperature in Fahrenheit(tmpf), Dew Point Temperature in Fahrenheit (dwpf), Relative Humidity in percentage (relh), Wind Speed in knots (sknt), Pressure altimeter in inches (alti), and visibility in miles (vsby). We chose these variables because each of these variables can have an impact on a biker's decision to choose or not to choose to bike and can significantly affect bike usage. We chose these variables to build a preliminary model and make predictions based off the model. The other variables like wind direction in degrees from north, sky level coverages, sky level altitudes, and wind gust had some missing observations. We believe that these variables don't affect bike usage much and not including them would not affect the model. 

