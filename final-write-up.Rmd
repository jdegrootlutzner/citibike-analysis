---
title: "Citi Bikes Analysis and Bike Demand Prediction"
author: "Julian DeGroot-Lutzner & Vikramaditya Salwan"
date: "12/14/2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo= TRUE, message = FALSE, warning = FALSE,
                      fig.width = 7, fig.height = 4,
                      fig.align = "center")
options(digits=4)

require(dplyr)
require(ggplot2)
require(RMySQL)
require(lubridate)
require(ggmap)
require(readr)
require(chron)
require(caret)
```
```{r, include=FALSE, cache = TRUE}
randomsample <- read_csv("data/randomsample.csv")

```
 
 
## Background and Motivation

Our project is inspired by the [Kaggle bike sharing demand competition](https://www.kaggle.com/c/bike-sharing-demand#description). Bike sharing is a service that has become popular within the last few years in many cities across the United States. Bikes are checked in and out of a network of stations that keep track of each ride, its start and end time, its start/end location, as well as information about subscribers such as gender and age. The Kaggle competition challenged users to predict the hourly bike usage of a test dataset by building a model on a training set combing hourly weather data with hourly bike share usage in Washington, DC.

We planned to first visualize and understand the data and then try to find if there are any inconsistencies in bike stations traffic. Is there a bike station that people take bikes from but do not take bikes back to? If there was an bike station that is a net exporter - meaning it has an excess in demand of bikes - we wanted to make recomendations on how often CitiBikes should manually transport bikes back to the station.

Instead of using the Kaggle data, we wanted to wrangle bike sharing data from a different city's bike share dataset so that we could get familiar with the process of preparing a dataset for analysis. We decided on the [Citibikes bike sharing dataset](https://www.citibikenyc.com/system-data) from New York City because there is data available on every ride from January 1st, 2013 to September 31, 2017. However, the dataset is very large, over 40 million observations, and the complete set cannot be ran in R. Our new goal became learning how to work with Big Data in R. 


## Working with Big Data in R

Big Data in R is when the data cannot fit in to memory. Instead, we stored the data in a SQL database and received the data using 'RMySQL' and 'dplyr.' We based our work process off of the [Working with Big Data in R webinar.](https://www.rstudio.com/resources/webinars/working-with-big-data-in-r/). The life cycle of a big data analysis project usually involves five parts. Subset (extract data to explore and work with), clarify (become familiar with the data and template a solution), develop (create a working model), productize (automate and integrate), and publish. In other words, first work with a smaller sample, then scale up the work to a larger dataset.

We worked on Dr. Jo Hardin's sever at the Pomona math department using MySQL. Since the server is within the Pomona network we needed to navigate both the Pomona ITS and Department firewall. Additonally, our limited permissions on our personal accounts made it difficult to update needed packages within R. These limitations forced us to learn, adapt, and change our project as needed.

The first difficulty in the project was uploading the Citibikes data to a SQL database. Citibikes publishes a zip file for each month of data so there were over 50 seperate files to download. We wrote scripts to download, alter, and write csv files from the the Citibikes server, to our server, and then onto the MySQL database. The difficulties served as a learning experience as we developed better skills in the command line as well as in writing Shell, Python, and SQL code. You can see some of this code in the setup-code folder. Unfortunately, as of now our complete process is not reproducible by a single code but we can come back and add more explanation on how to use this code later.

Dplyr allows integration with many different databases. We originally tried running code on the complete >40 million dataset. However, the SQL queries took too long for us to create a reasonable project within our relatively short time frame. For example, some queries would take more than a day. In the future we could use other big data techniques like MapReduce to work more quickly. Instead we used the aforementioned big data work schedule of subsetting and scaling. First we took a random sample of 500,000 observations - approxiametly 1 % - from the original dataset.

## Understanding the Random Sample

### Exploratory Graphs on Ride Use

We wrote a lot of code, but we decided to use as little code as possible in our write up so that it is easier to read. If you're interested in learning how to make cool graphs like these please download our Rmd file and take a look for yourself. 

Here are some graphs that should help you get an understanding of CitiBike usage. After the graphs is a write up about some of the takeaways we found.

```{r, cache =TRUE, echo=FALSE}
# Parsing all the start times into one format
mdy <- mdy_hms(randomsample$starttime) 
ymd <- ymd_hms(randomsample$starttime) 
f1 <- mdy_hm(randomsample$starttime)
mdy[is.na(mdy)] <- ymd[is.na(mdy)] # some dates are ambiguous, here we give 
randomsample$starttime <- mdy
randomsample$starttime[is.na(randomsample$starttime)] <- f1[is.na(randomsample$starttime)]

# Parsing all the end times into one format 
mdy <- mdy_hms(randomsample$stoptime) 
ymd <- ymd_hms(randomsample$stoptime) 
f1 <- mdy_hm(randomsample$stoptime)
mdy[is.na(mdy)] <- ymd[is.na(mdy)] 
randomsample$stoptime <- mdy
randomsample$stoptime[is.na(randomsample$stoptime)] <- f1[is.na(randomsample$stoptime)]

# Flooring the Hour
randomsample$starttime <- floor_date(randomsample$starttime,unit = "hour")
# Trying to Find Day of the Week
randomsample <- randomsample %>%
  mutate(WeekDay=wday(as.Date(starttime), label=TRUE)) %>%
  mutate(WorkingDay=ifelse((WeekDay=="Mon"|WeekDay=="Tues"|WeekDay=="Wed"|WeekDay=="Thurs"|WeekDay=="Fri"),"Yes","No")) 

# Trying to find the holidays - Not quite working. Need to do this manually!
randomsample <- randomsample %>%
  mutate(holiday=is.weekend(starttime))

# Joining the weather data and the random sample
combined <- randomsample

# Number of Stations Every Year
rs <- randomsample %>%
  mutate(year=year(starttime),monyear=substr(starttime,1,7)) %>%
  group_by(year,usertype,monyear) %>%
  summarise(stations=n_distinct(start.station.id),rides=n())
combined$WorkingDay[combined$WorkingDay=="Yes"]<- "Weekday"
combined$WorkingDay[combined$WorkingDay=="No"]<- "Weekend"

# Exploratory Analysis -
combined1 <- combined%>% 
  mutate(year=year(starttime),
         month=month(starttime),
         monyear=substr(starttime,1,7)) %>%
  group_by(monyear,year)%>%
  summarise(usage=n())

combined1$year <- as.factor(combined1$year)

# Graph to See the Monthly Usage
ggplot(combined1,aes(x=monyear,y=usage,fill=year))+
  geom_bar(stat="identity")+ 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+
  ggtitle("Bike Usage in Different Months (July 2013 to September 2017)")+
  ylab("Number of Bike Rides")+
  xlab("Month and Year")

# Wrangling the data
combined2 <-  combined%>% 
  mutate(year=year(starttime),
         week=week(starttime),
         monyear=substr(starttime,1,7)) %>%
  group_by(week,year)%>%
  summarise(usage=n(),MedianDuration=median(tripduration)) %>%
  mutate(wy=paste(week,"-",year)) %>%
  filter(year==2016) %>%
  arrange(year)

# Graph to See the Usage in 2016
ggplot(combined2,
       aes(x=week,
           y=usage,
           fill=MedianDuration))+
  geom_bar(stat="identity")+
  theme(axis.text.x = 
          element_text(angle = 90, hjust = 1))+
  ggtitle("Weekly Bike Rides in 2016")+ylab("Usage")+xlab("Week")

combined4 <- combined%>% 
  mutate(year=year(starttime),
         month=month(starttime),
         hour=hour(starttime)) %>%
  filter(!is.na(usertype))%>%
  group_by(hour,gender,usertype,WeekDay)%>%
  summarise(usage=n(),MedianDuration=median(tripduration))
# The Number of Rides for every hour 

#Graph for Bike Usage on Different Day
ggplot(combined4,aes(x=hour,y=usage,fill=WeekDay))+
  geom_bar(stat="identity")+
  facet_grid(~WeekDay)+
  xlab("Hour of the Day")+
  ylab("Number of Bike Rides")+
  ggtitle("Bike Usage on Different Days")

combined7 <- combined%>% 
  mutate(year=year(starttime),month=month(starttime),hour=hour(starttime)) %>%
  filter(!is.na(usertype))%>%
  group_by(hour,gender,usertype,WorkingDay)%>%
  summarise(usage=n(),MedianDuration=median(tripduration))
# The Number of Rides for every hour - 

ggplot(combined7,aes(x=hour,y=usage,fill=WorkingDay))+
  geom_bar(stat="identity")+
  facet_grid(~WorkingDay)+
  scale_fill_manual(values=c("orange", "lightgreen"))+
  xlab("Hour of the Day")+
  ylab("Number of Bike Rides")+
  ggtitle("Bike Usage on Weekdays versus Weekends")

# Making Different Levels for the Time of the Day
combined4 <- combined4 %>%
  mutate(EarlyMorning=ifelse(hour=="3"|hour=="4"|hour=="5"|hour=="6",1,0)) %>%
  mutate(Commuting=ifelse((hour=="7"|hour=="8"|hour=="9"),1,0)) %>%
  mutate(DayTime=ifelse((hour=="10"|hour=="11"|hour=="12"|hour=="13"|hour=="14"|hour=="15"),1,0)) %>%
  mutate(Evening=ifelse((hour=="16"|hour=="17"|hour=="18"|hour=="19"|hour=="20"),1,0)) %>%
  mutate(Night=ifelse((hour=="21"|hour=="22"|hour=="23"|hour=="0"|hour=="1"|hour=="2"),1,0))

combined5 <- combined%>% 
    filter(!is.na(birth.year)) %>%
  mutate(year=year(starttime),
         month=month(starttime),
         monyear=substr(starttime,1,7),
         age = 2017-birth.year) %>%
  group_by(monyear,year,age,usertype,gender)%>%
  summarise(usage=n())%>%
  filter(age<100,gender!=0)

combined5$gender <- as.factor(combined5$gender)

ggplot(combined5,aes(x=age,y=usage,fill=gender))+
  geom_bar(stat="identity")+facet_grid(~year)+
  ggtitle("Yearly Bike Usage by Age and Gender")+
  ylab("Number of Bike Rides")+
  xlab("Age")+
  scale_fill_manual(values=c("skyblue", "lightgreen"))+
  geom_vline(xintercept = 75,alpha=0.5)+
  geom_vline(xintercept = 25,alpha=0.5)

ggplot(combined5,aes(x=year,y=usage,fill=gender))+
  geom_bar(stat="identity")+
  theme(axis.text.x = 
          element_text(angle = 90, hjust = 1))+
  facet_grid(~gender)+
  ggtitle("Yearly Bike Usage by Gender")+
  ylab("Number of Bike Rides")+
  xlab("Year")+
  scale_fill_manual(values=c("skyblue", "lightpink"))
```

Based on a preliminary exploratory analysis the following insights were found which could be useful for Citi Bikes: 

1) In the graph above blue represents males and pink represents females. It was found that bike usage for both females and males has increased each year since the initiation of this bike sharing project in May 2013 and since the data became available in July 2013. After the missing observations for gender were removed, it was found that bikes are used predominantly by males. We believe this trend is unexpected and needs to be corrected. We posit that female prefer walking, which is why the bike usage for females is not very high as compared to bike usage for males. The 2017 data is only available till September, which is why when compared to 2016 it shows a decline.

2) It was found that bike usage has increased the most for the 25-50 years age group. The bike usage for the age group of 50-75 has increased significantly too, while the usage for the age group less than 25 increased too. The bike usage for the age group of 75 years or older has not increased much. Based on this information, we believe that the target market for Citi Bikes is 25 to 50 year olds. 

3) In the year 2016 bike usage peaked from week 21 to week 44. This corresponds to the dates from May 16 to October 31. This is because bike usage is generally higher during the summer and fall months. It was also found that the median bike ride is longer during these months as the weather does not act as an impediment for bikers. The highest median bike ride was in week 36 (12 minutes and 9 seconds), while the lowest was in week 7 (8 minutes and 8 seconds). Hence weather does play a part in biker's decision whether to bike or not. However, it remains to be seen which of the six weather predictors play the most important role in a user's decision to bike or not. 

4) Based on the first graph, it can be seen that the overall popularity of this bike sharing system has surged significantly each year. It has increased each year since its inception. The number of bike rides continue to grow, but the number of bike rides in the last four months in 2013 are higher than the last four months in 2014, albeit not by a big amount. This may have been because of the weather conditions. The average temperature in September 2013 was 67.2 degree Farenheit, 2 degrees higher than the average temperature in September 2014, which could have translated in the marginally lower number of bike rides in in September 2014 than in September 2013. Similarly, the average temperature in December 2013 was 2 degrees lower than the average temperature in December 2014. We hypothesize that weather conditions could be the reason for this slight decrease in bike rentals. However, it is still not quite clear about why the number of bike rides rented in October and November 2014 were marginally lower than in October and November 2013. 

5) The bike usage is significantly higher during the weekdays than during weekends. This could be because the bikes are predominantly used by commuters commuting to work on weekdays. This drop is the highest during the commuting hours from 7am to 9am and from 4pm to 7pm, which makes us believe that commuters account for a majority of the bike rides in New York City. Based on this information, Citi Bikes could shift its major focus to commuters instead of any other group. This could be undertaking or promoting campaigns to bike to work, which would increase their annual memberships. 

It is important to note that the exploratory analysis is based on a sample of 500,000 observations, which is 1% of the total population. This sample is representative of the population because of its large size. 

## Station Analysis

After getting a better sense of the data, we will return to one of the orignal questions we had. Are there any bike stations that have inconsistencies between bike supply and bike demand? This part of the exploratory analysis moves to the stations. 

First lets see where the stations are located in New York City.

```{r,cache=TRUE, echo=FALSE}
# wrangling data for graphs
start_sums <- randomsample %>%
  group_by(start.station.id) %>%
  summarize(start.station.longitude = mean(start.station.longitude),
            start.station.latitude = mean(start.station.latitude),
            median.time.out = median(tripduration, na.rm= TRUE),
            start.count = n()) %>% 
  ungroup()

# the map used for plots
center.citibikes <- c(
  lon = mean(randomsample$start.station.longitude),
  lat = mean(randomsample$start.station.latitude))
mymap <- get_map(location = center.citibikes,
                 maptype = "roadmap",
                 zoom = 12)
# heatmap
 ggmap(mymap, extent = "panel", maprange=FALSE) +
  geom_density2d(data = start_sums, 
                           aes(x = start.station.longitude,
                               y = start.station.latitude)) +
  stat_density2d(data = start_sums, 
                           aes(x = start.station.longitude,
                               y = start.station.latitude, fill = ..level.., alpha = ..level..),
                 size = 0.01, bins = 16, geom = 'polygon') +
  scale_fill_gradient(low = "green", high = "red") +
  scale_alpha(range = c(0.00, 0.25), guide = FALSE) +
  theme(legend.position = "none", axis.title = element_blank(), text = element_text(size = 12)) +
  ggtitle("Heatmap of Location of Bike Stations")
```

As you can see most bike stations are centered in lower Manhatten. Now lets get a different but related view of Citibikes as shown by the number of rides per station.
```{r, cache=TRUE, echo=FALSE}
ggmap(mymap) +  geom_point(data = start_sums, 
                           aes(x = start.station.longitude,
                               y = start.station.latitude,
                               fill = "red", alpha = start.count),
                           size = 1, shape = 21) +
  guides(fill=FALSE, alpha=FALSE, size=FALSE) +
  ggtitle("Number of Rides per Station", subtitle = "Darker Red = More Rides")
```

As we can see most bike rides happen in downtown Manhatten. The rides really slim out over the east and west side of Central park. We can see a slight increase of red in Brooklyn on the opposite side of the Brooklyn Bridge. We must keep in mind that we did not account for the number of years that the station was around. Now let's take a look at the median trip duration.
```{r, cache=TRUE, echo=FALSE}
ggmap(mymap) +  geom_point(data = start_sums, 
                           aes(x = start.station.longitude,
                               y = start.station.latitude,
                               fill = "red", 
                               alpha = median.time.out),
                           size = 1, shape = 21) +
  guides(fill=FALSE, alpha=FALSE, size=FALSE) + 
  ggtitle("Median Trip Duration to Station",
          subtitle = "Darker Red = Longer Trip")
```

The one red dot could be an outlier and is worth investigating more but we chose to go in a different direction.

At this point we moved in to our investigation of whether a station was a net importer or exporter. To figure this out we calculated the sums of the number of trips in and out of a station for each station. We then found the difference between the start and the finish or the net change. The difference (start - finish) is positive if more bikes start at a station than end there, and is negative if more bikes end at a station than start there. *I confused myself and probably should have rethought about the change as in and out and then I would have gotten finish - start or in - out. Anyways, please deal with this confusion*

We normalized the difference by dividing it by the total station trips and we filtered the stations to only look at the top ten percent of the most popular stations (the stations with the most bike rides). Looking back at our metric, we could have adjusted for how long the bike station has been around and looked at how the usage has changed overttime. For example, did extreme popularity of one station spark a new station near it that helped provide more supply of bikes? That's a question we should look at in the future.

```{r,cache= TRUE, echo=FALSE}
end_sums <- randomsample %>%
  group_by(end.station.id) %>%
  summarize(end.station.longitude = mean(end.station.longitude),
            end.station.latitude = mean(end.station.latitude),
            median.time.in = median(tripduration, na.rm = TRUE),
            end.count = n()) %>% 
  ungroup()

colnames(start_sums)[1]<- c("id")
colnames(end_sums)[1] <- c("id")
joined_data<-inner_join(start_sums,end_sums,by="id")
joined_data <- joined_data %>%
mutate(difference = start.count - end.count,
       station.latitude = 
         (start.station.latitude+ end.station.latitude)/2,
       station.longitude = 
         (start.station.longitude + end.station.longitude)/2,
       total.ride.count = start.count + end.count) %>%
  mutate(normalized.difference =
           difference/(start.count+end.count),
         positive.difference = difference>0) %>%
  select(-start.station.latitude,
         -start.station.longitude,
         -end.station.longitude,
         -end.station.latitude) 
```

Here is a list of the top exporters - more bikes start from these stations than end at these stations. They need more bikes. 

```{r, cache = TRUE, echo=FALSE}
top_ten_perc <- joined_data$total.ride.count %>% 
  quantile(0.90)

biggest_differences <- joined_data %>% 
  filter(total.ride.count >= top_ten_perc) %>%
  arrange(desc(normalized.difference)) %>%
head(10)

biggest_differences %>% 
    select(id, start.count, end.count, 
           difference, normalized.difference)

```

Here is a list of the top importers - more bikes **end** at these stations than start in these stations. They have excess bikes.

```{r, cache=TRUE, echo=FALSE}
smallest_differences <- joined_data %>% 
  filter(total.ride.count >= top_ten_perc) %>%
  arrange(normalized.difference) %>%
head(10)
smallest_differences %>%
  select(id, start.count, end.count, 
         difference, normalized.difference)
```

Tables of data are no fun! Let's see the data visualized.

```{r, cache=TRUE, echo = FALSE}
ggmap(mymap) +
  guides(fill=FALSE, alpha=FALSE, size=FALSE) +
  geom_point(data=biggest_differences,
             aes(x = station.longitude,
                 y = station.latitude,
                 fill = "blue",
                 alpha = 1.0),
             size = 3, shape = 21) +
    geom_point(data=smallest_differences,
             aes(x = station.longitude,
                 y = station.latitude,
                 fill = "red",
                 alpha = 1.0),
             size = 3, shape = 21)
```

As you can see most of the most popular importers are in the lower side of Manhatten. Most of the popular exporters are closer to time square. Although the analysis is not perfect. We decided to move on and make a predictive models on the top two stations. We chose our importer to be station 432, which is in Alphabet City (at E 7 St & Avenue A near Washington Square Park), and our exporter to be station 521, which is in Midtown between the Empire State Building and Lincoln Tunnel (at 8 Ave & W 31 St N). Shown below:

```{r, cache = TRUE, echo=FALSE}
# Narrow down the two highest normalized difference stations
head(joined_data)
stations_of_interest <- joined_data %>% 
  filter(id == 432 | id == 521)

# Plot to show where they are
ggmap(mymap) +  
  geom_point(data=stations_of_interest,
             aes(x = station.longitude,
                 y = station.latitude,
                 fill = positive.difference,
                 alpha = 1.0),
             size = 4, shape = 21) +
  guides(fill=FALSE, alpha=FALSE, size=FALSE)  +
  ggtitle("Stations of Interest",
          subtitle = "Station 432 in Red, Station 521 in Blue")
```

Let's move on to our model.

## Building a Model

```{r, cache=TRUE, echo=FALSE}
station_432 <- read.csv("data/station-432.csv")
station_521 <- read.csv("data/station-521.csv")
Weather_NYC <- read_csv("data/Weather_NYC.csv")
```

We went back to our SQL server and now pulled a new subsample: all rides that went to and from our selected stations. At this point we will now combine the data from Citibikes and [historic Weather Data](https://mesonet.agron.iastate.edu/request/download.phtml?network=NY_ASOS#) to create predictive models for stations changes. We combined the data by hour so that each hour has the 

```{r, cache= TRUE, echo= FALSE}
# Parsing all the start times into one format
mdy <- mdy_hms(station_432$starttime) 
ymd <- ymd_hms(station_432$starttime) 
f1 <- mdy_hm(station_432$starttime)
mdy[is.na(mdy)] <- ymd[is.na(mdy)] 
station_432$starttime <- mdy
station_432$starttime[is.na(station_432$starttime)] <- f1[is.na(station_432$starttime)]

# Parsing all the start times into one format
mdy <- mdy_hms(station_521$starttime) 
ymd <- ymd_hms(station_521$starttime) 
f1 <- mdy_hm(station_521$starttime)
mdy[is.na(mdy)] <- ymd[is.na(mdy)] 
station_521$starttime <- mdy
station_521$starttime[is.na(station_521$starttime)] <- f1[is.na(station_521$starttime)]
```

We wrote code to wrangle the data so that it is ready to make interesting graphs about each station but we never had time to make the graphs. The start times, end times and dates were in different formats. We made the Citi Bikes date time objects and the weather data date time objects into one format using the tidyverse package lubridate. The code can be found in station-analysis.Rmd. 

```{r, cache= TRUE, echo=FALSE}

# took out rides that looped to the same station because these 
# rides don't impact our prediction model
median_ <- function(...) median(..., na.rm=T)

# hourly sums of station 521
hourly_sums_521 <- station_521 %>% 
  select( starttime, start.station.id,
         end.station.id) %>%
  mutate(starttime = floor_date(starttime, "hour"),
         started.here = (start.station.id == 521),
         ended.here = (end.station.id == 521)) %>%
  group_by(starttime) %>%
  summarize(total.trips.started.521 =
              sum(started.here & !ended.here),
            total.trips.ended.521 =
              sum(!started.here & ended.here)) %>%
  mutate(net.change.521 = 
  total.trips.started.521 -total.trips.ended.521 )

# hourly sums for station 432
hourly_sums_432 <- station_432 %>% 
  select(starttime, start.station.id,
         end.station.id) %>%
  mutate(starttime = floor_date(starttime, "hour"),
         started.here = (start.station.id == 432),
         ended.here = (end.station.id == 432)) %>%
  group_by(starttime) %>%
  summarize(total.trips.started.432 =
              sum(started.here & !ended.here),
            total.trips.ended.432 =
              sum(!started.here & ended.here)) %>%
  mutate(net.change.432 = 
  total.trips.started.432 -total.trips.ended.432 )
```

```{r,cache=TRUE}
# Choosing important variables
Weather_NYC <- Weather_NYC %>%
  select(valid, tmpf, dwpf, relh, vsby)
```
Rationale for Choosing the weather data variables:
The variables relating to the weather data chosen are air temperature in Fahrenheit(tmpf), Dew Point Temperature in Fahrenheit (dwpf), Relative Humidity in percentage (relh), and visibility in miles (vsby). We chose these variables because each of these variables can have an impact on a biker's decision to choose or not to choose to bike and can significantly affect bike usage. We chose these variables to build a preliminary model and make predictions based off the model. The other variables like wind direction in degrees from north, sky level coverages, sky level altitudes, and wind gust had some missing observations. We believe that these variables don't affect bike usage much and not including them would not affect the model.

```{r, cache =TRUE, echo=FALSE}
Weather_NYC <- Weather_NYC %>% 
  mutate(valid = ymd_hms(valid)) %>%
  filter(minute(valid)=="51")  %>%
  mutate(valid = ceiling_date(valid, unit = "hour"),
         Month=month(valid)) %>%
  mutate( summer=ifelse(Month=="6"|Month=="7"|Month=="8",1,0),
          spring=ifelse((Month=="3"|Month=="4"|Month=="5"),1,0),
          winter=ifelse((Month=="1"|Month=="2"|Month=="12"),1,0),
          fall=ifelse((Month=="9"|Month=="10"|Month=="11"),1,0),
          day.of.week=wday(valid),
          hour = hour(valid)) %>%
  mutate(week.day=
           ifelse(day.of.week > 1 & day.of.week < 7,TRUE,FALSE),
         weekend.day=
           ifelse(day.of.week == 1 | day.of.week == 7,TRUE,FALSE),
         EarlyMorning=
           ifelse(hour=="0"|hour=="1"|hour=="2"|hour=="3"|hour=="4"|hour=="5"|hour=="6",1,0),
         Commuting=
           ifelse((hour=="7"|hour=="8"|hour=="9"),1,0),
         DayTime=
           ifelse((hour=="10"|hour=="11"|
              hour=="12"|hour=="13"|hour=="14"|hour=="15"),1,0),
         Evening=ifelse((hour=="16"|hour=="17"|
                           hour=="18"|hour=="19"),1,0),
         Night=ifelse((hour== "20"|
                         hour=="21"|hour=="22"|hour=="23"),1,0),
         starttime = valid) %>% 
  select(-valid, -hour, -Month, -day.of.week)

```
Based on the graphs we made for the original exploratory analysis we decided to group hour into categories of early morning, morning commute, daytime, and evening commute, and night. 

We also engineered variables for weekday vs. weekend and all of the seasons.

```{r,cache=TRUE, echo=FALSE}
# combine weather station and weather data
station_432_combined <- hourly_sums_432 %>%
  select(starttime, net.change.432 ) %>%
  inner_join(Weather_NYC, by= "starttime" )

# combine weather station and weather data
station_521_combined <- hourly_sums_521 %>%
  select(starttime, net.change.521 ) %>%
  inner_join(Weather_NYC, by= "starttime" )

# only a few rows were missing data so we removed them
station_521_combined <- station_521_combined[complete.cases(station_521_combined),] %>%
  select(-starttime)

station_432_combined <- station_432_combined[complete.cases(station_432_combined),] %>%
  select(-starttime)
```
A small part of the weather data was missing fields - roughly 1% - so we decided to remove these rows. 

### Random Forest Regression and Classification

```{r, cache=TRUE, echo= FALSE}
set.seed(4747)
inTrain <- 
  createDataPartition(station_521_combined$net.change.521,
                      p = 0.7, list=FALSE)
training.521 <- station_521_combined[inTrain, ]
testing.521 <- station_521_combined[-inTrain,]

set.seed(4747)
rf.model <- train(net.change.521~.,
                  data = training.521,
                  method = "rf",
                  trControl=trainControl(method="oob"),
                  ntree = 200,
                  tuneGrid=data.frame(mtry=6))
prediction <- predict(rf.model, testing.521)

RMSE(prediction, testing.521$net.change.521)
```
Addtionally, let's see let's look at how accurate the model was at classifying if the difference was positive or negative (export or import).
```{r, cache = TRUE, echo=FALSE}
actual_positive_change <- (testing.521$net.change.521 > 0)
pred_positive_change <- (prediction > 0)
confusionMatrix(pred_positive_change, actual_positive_change)
```
Does our model do better than predicting all of the differences as negative ? (All of the hourly changes as import for station 521)
```{r, cache = TRUE, echo=FALSE}
all_true <- (prediction < -10000)
confusionMatrix(all_true, actual_positive_change)
```
Yes, our classification error reat is better than baseline (classifying all the hourly changes for station 521 as net import). However, the RMSE is still very high. 16.86 is too high of a Root Mean Squared Error to be effective in predicting when a station might need bikes moved to it. The RMSE means that the errors are off by almost 17 bikes. That means that the prediction could predict 16 bikes left a station but actually no bikes left the station. In a future model, we should just predict the hourly demand (not hourly change) for each station and then create information from that.

### SVM Linear Classification 

```{r, cache = TRUE, echo = FALSE}
station.521.svm<- station_521_combined %>%
  mutate(net.change.521 = ifelse(net.change.521 > 0, "P","N")) 

set.seed(4747)
inTrain <- 
  createDataPartition(station.521.svm$net.change.521,
                      p = 0.7, list=FALSE)
training.svm.521 <- station.521.svm[inTrain, ]
testing.svm.521 <- station.521.svm[-inTrain,]
head(training.svm.521)
head(testing.svm.521)
set.seed(47)
svm.linear.model <- train(net.change.521~., data = training.svm.521, method="svmLinear", 
                 trControl = trainControl(method="cv"),
                 tuneGrid= expand.grid(C= (0.1)),
                 preProcess = c("center", "scale"))

svm.linear.pred <- predict(svm.linear.model, testing.svm.521)

confusionMatrix(svm.linear.pred, testing.svm.521$net.change.521)

```
The accuracy rate for the Support Vector Machines model was slightly lower than Random Forest by about 2 % but still better than the base line prediction.

## Final Remarks

Our last modeling step was rushed. We did not spend the time graphing the data to better visualize how weather interacts with bike demand. Additionally, we did not spend enough time choosing variables and optimizing the models. Building models on large sets of data is time consuming! However, we still wanted to incorporate it to show how the data analysis life cycle might work. **We started with a very large dataset, then made a random sample to learn about the overall data. From the randomized sample we discovered that we wanted to look into a new subset of data (on stations we thought were of interest). We began to build a model on this data but found that it was not very effective at predicting our desired information (when to move bikes from one station to another).** The next step would be to go back to the drawing board and come up with a better way to approach our orignal goal. Once our model works we could scale it up to the larger dataset and then finish the cycle of the data analysis! Overall, this project, despite its limitations, was a great learning experience for the both of us. We ended up learning five new things throughout the course of this project namely big data analysis, SQL, visualizations using ggmaps, shell, and wrangling using lubridate. 
